{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Batch of images shape: torch.Size([64, 3, 32, 32])\n",
      "Batch of labels: tensor([88, 65, 10, 86, 15, 94, 21, 22, 80, 53, 42, 67, 59, 79, 85, 35, 57, 45,\n",
      "        87, 97, 34, 85, 77, 74, 67,  8, 67, 96, 97,  4, 32, 18, 89, 61, 70, 16,\n",
      "        31, 31, 52, 45, 26, 73, 38, 18, 21, 21, 55, 62, 94, 48, 97, 55, 48, 88,\n",
      "        20, 43,  9, 71, 89, 18, 17, 41, 73, 81])\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the transformations for the dataset (you can modify this as needed)\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Random color adjustments\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])\n",
    "\n",
    "\n",
    "# Download the CIFAR-100 training dataset\n",
    "train_dataset = datasets.CIFAR100(\n",
    "    root='./cifar-100-python',       # Change this path if needed\n",
    "    train=True,          # Set to True to download the training set\n",
    "    download=True,       # Set to True to download if not already downloaded\n",
    "    transform=transform  # Apply transformations\n",
    ")\n",
    "##### Hyper-parameters\n",
    "batch_size = 64 # \n",
    "\n",
    "# Create a DataLoader for batch processing\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,      # Batch size (you can modify this as needed)\n",
    "    shuffle=True        # Shuffle data for training\n",
    ")\n",
    "\n",
    "# Example: Accessing one batch of images and labels\n",
    "images, labels = next(iter(train_loader))\n",
    "print(f\"Batch of images shape: {images.shape}\")\n",
    "print(f\"Batch of labels: {labels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Test set size: 10000 images\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define transformations for the test set\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2761))  # CIFAR-100 mean and std\n",
    "])\n",
    "\n",
    "# Download the CIFAR-100 test dataset\n",
    "test_dataset = torchvision.datasets.CIFAR100(\n",
    "    root='./cifar-test-data',  # Specify the path where data will be stored\n",
    "    train=False,    # Set to False to download the test set\n",
    "    download=True,  # Automatically download the dataset if it's not present\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Access the test data loader\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=64,  # Set your preferred batch size\n",
    "    shuffle=False   # Test set is typically not shuffled\n",
    ")\n",
    "\n",
    "print(f\"Test set size: {len(test_dataset)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # Core PyTorch library for tensor operations\n",
    "from torchvision import datasets, transforms  # Datasets and transformations for computer vision\n",
    "import torch.nn as nn  # Neural network components\n",
    "import torch.optim as optim  # Optimization algorithms\n",
    "import numpy as np  # Numerical operations\n",
    "import matplotlib.pyplot as plt  # Plotting\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset  # Data handling utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64           # Batch size for data loading\n",
    "num_epochs = 100          # Number of training epochs\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the model\n",
    "class Cifar100Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cifar100Classifier, self).__init__()\n",
    "        self.cl1 = nn.Conv2d(3, 32, kernel_size=3, padding=1, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.cl2 = nn.Conv2d(32, 64, kernel_size=3, padding=1, stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.cl3 = nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.cl4 = nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(256 * 2 * 2, 512)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(512, 100)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu(self.bn1(self.cl1(x))))\n",
    "        x = self.pool2(self.relu(self.bn2(self.cl2(x))))\n",
    "        x = self.pool3(self.relu(self.bn3(self.cl3(x))))\n",
    "        x = self.pool4(self.relu(self.bn4(self.cl4(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and move to device\n",
    "model = Cifar100Classifier().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # CrossEntropy includes softmax\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_batches = []\n",
    "\n",
    "  # Switch to evaluation mode and turn off gradient calculation\n",
    "  # since parameters are not updated during testing.\n",
    "    with torch.no_grad():\n",
    "        for images_batch, labels_batch in data_loader:\n",
    "            outputs = model(images_batch)\n",
    "            # The predicted label is the output with the highest activation.\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels_batch.size(0)\n",
    "            correct += (predicted == labels_batch).sum().item()\n",
    "\n",
    "            # Use provided criterion to calculate the loss for the mini batch\n",
    "            # Append the mini-batch loss to loss_batches array\n",
    "            batch_loss = criterion(outputs, labels_batch)\n",
    "            loss_batches.append(batch_loss.item())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    loss = np.mean(loss_batches)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MarkKogan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/60 - Train Loss: 3.564788, Train Acc: 14.52%\n",
      "            - Test Loss: 3.464212, Test Acc: 16.40%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 16.40%\n",
      "Epoch 02/60 - Train Loss: 3.299836, Train Acc: 18.05%\n",
      "            - Test Loss: 3.194793, Test Acc: 19.77%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 19.77%\n",
      "Epoch 03/60 - Train Loss: 2.970397, Train Acc: 25.08%\n",
      "            - Test Loss: 2.863430, Test Acc: 26.96%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 26.96%\n",
      "Epoch 04/60 - Train Loss: 2.742107, Train Acc: 29.66%\n",
      "            - Test Loss: 2.663792, Test Acc: 30.88%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 30.88%\n",
      "Epoch 05/60 - Train Loss: 2.759492, Train Acc: 30.00%\n",
      "            - Test Loss: 2.704371, Test Acc: 31.08%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 31.08%\n",
      "Epoch 06/60 - Train Loss: 2.477486, Train Acc: 35.76%\n",
      "            - Test Loss: 2.412498, Test Acc: 36.52%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 36.52%\n",
      "Epoch 07/60 - Train Loss: 2.480922, Train Acc: 34.71%\n",
      "            - Test Loss: 2.442128, Test Acc: 35.93%\n",
      "------------------------------------------------------------\n",
      "Epoch 08/60 - Train Loss: 2.414414, Train Acc: 36.38%\n",
      "            - Test Loss: 2.359223, Test Acc: 37.62%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 37.62%\n",
      "Epoch 09/60 - Train Loss: 2.265768, Train Acc: 40.22%\n",
      "            - Test Loss: 2.218224, Test Acc: 40.39%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 40.39%\n",
      "Epoch 10/60 - Train Loss: 2.205039, Train Acc: 40.55%\n",
      "            - Test Loss: 2.162703, Test Acc: 40.91%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 40.91%\n",
      "Epoch 11/60 - Train Loss: 2.163308, Train Acc: 42.32%\n",
      "            - Test Loss: 2.136343, Test Acc: 42.70%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 42.70%\n",
      "Epoch 12/60 - Train Loss: 2.061519, Train Acc: 44.75%\n",
      "            - Test Loss: 2.013759, Test Acc: 45.33%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 45.33%\n",
      "Epoch 13/60 - Train Loss: 2.041776, Train Acc: 44.92%\n",
      "            - Test Loss: 2.004877, Test Acc: 46.32%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 46.32%\n",
      "Epoch 14/60 - Train Loss: 1.947405, Train Acc: 47.33%\n",
      "            - Test Loss: 1.929389, Test Acc: 47.49%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 47.49%\n",
      "Epoch 15/60 - Train Loss: 1.895345, Train Acc: 48.51%\n",
      "            - Test Loss: 1.916052, Test Acc: 47.82%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 47.82%\n",
      "Epoch 16/60 - Train Loss: 1.881182, Train Acc: 48.74%\n",
      "            - Test Loss: 1.880539, Test Acc: 48.57%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 48.57%\n",
      "Epoch 17/60 - Train Loss: 1.847658, Train Acc: 49.81%\n",
      "            - Test Loss: 1.855988, Test Acc: 49.32%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 49.32%\n",
      "Epoch 18/60 - Train Loss: 1.785828, Train Acc: 51.15%\n",
      "            - Test Loss: 1.809981, Test Acc: 50.87%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 50.87%\n",
      "Epoch 19/60 - Train Loss: 1.727940, Train Acc: 52.42%\n",
      "            - Test Loss: 1.780199, Test Acc: 51.61%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 51.61%\n",
      "Epoch 20/60 - Train Loss: 1.689085, Train Acc: 53.19%\n",
      "            - Test Loss: 1.757431, Test Acc: 52.09%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 52.09%\n",
      "Epoch 21/60 - Train Loss: 1.690152, Train Acc: 53.30%\n",
      "            - Test Loss: 1.756057, Test Acc: 51.96%\n",
      "------------------------------------------------------------\n",
      "Epoch 22/60 - Train Loss: 1.615321, Train Acc: 54.94%\n",
      "            - Test Loss: 1.724606, Test Acc: 52.19%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 52.19%\n",
      "Epoch 23/60 - Train Loss: 1.603309, Train Acc: 55.77%\n",
      "            - Test Loss: 1.710550, Test Acc: 53.61%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 53.61%\n",
      "Epoch 24/60 - Train Loss: 1.591966, Train Acc: 55.43%\n",
      "            - Test Loss: 1.713955, Test Acc: 53.17%\n",
      "------------------------------------------------------------\n",
      "Epoch 25/60 - Train Loss: 1.528888, Train Acc: 57.02%\n",
      "            - Test Loss: 1.671471, Test Acc: 53.98%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 53.98%\n",
      "Epoch 26/60 - Train Loss: 1.530165, Train Acc: 56.87%\n",
      "            - Test Loss: 1.690958, Test Acc: 53.61%\n",
      "------------------------------------------------------------\n",
      "Epoch 27/60 - Train Loss: 1.484479, Train Acc: 57.97%\n",
      "            - Test Loss: 1.647599, Test Acc: 54.31%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 54.31%\n",
      "Epoch 28/60 - Train Loss: 1.485495, Train Acc: 58.11%\n",
      "            - Test Loss: 1.648839, Test Acc: 54.54%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 54.54%\n",
      "Epoch 29/60 - Train Loss: 1.451646, Train Acc: 58.97%\n",
      "            - Test Loss: 1.637601, Test Acc: 54.68%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 54.68%\n",
      "Epoch 30/60 - Train Loss: 1.455680, Train Acc: 58.87%\n",
      "            - Test Loss: 1.650299, Test Acc: 54.75%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 54.75%\n",
      "Epoch 31/60 - Train Loss: 1.431659, Train Acc: 59.68%\n",
      "            - Test Loss: 1.600084, Test Acc: 55.67%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 55.67%\n",
      "Epoch 32/60 - Train Loss: 1.394103, Train Acc: 60.59%\n",
      "            - Test Loss: 1.592175, Test Acc: 55.88%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 55.88%\n",
      "Epoch 33/60 - Train Loss: 1.384204, Train Acc: 60.87%\n",
      "            - Test Loss: 1.585356, Test Acc: 56.11%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 56.11%\n",
      "Epoch 34/60 - Train Loss: 1.385298, Train Acc: 60.78%\n",
      "            - Test Loss: 1.581355, Test Acc: 56.34%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 56.34%\n",
      "Epoch 35/60 - Train Loss: 1.402948, Train Acc: 60.14%\n",
      "            - Test Loss: 1.608966, Test Acc: 55.52%\n",
      "------------------------------------------------------------\n",
      "Epoch 36/60 - Train Loss: 1.306460, Train Acc: 62.66%\n",
      "            - Test Loss: 1.533767, Test Acc: 57.51%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 57.51%\n",
      "Epoch 37/60 - Train Loss: 1.346047, Train Acc: 62.01%\n",
      "            - Test Loss: 1.552608, Test Acc: 57.22%\n",
      "------------------------------------------------------------\n",
      "Epoch 38/60 - Train Loss: 1.306722, Train Acc: 62.75%\n",
      "            - Test Loss: 1.562968, Test Acc: 57.15%\n",
      "------------------------------------------------------------\n",
      "Epoch 39/60 - Train Loss: 1.287664, Train Acc: 63.16%\n",
      "            - Test Loss: 1.540640, Test Acc: 57.24%\n",
      "------------------------------------------------------------\n",
      "Epoch 40/60 - Train Loss: 1.275357, Train Acc: 63.44%\n",
      "            - Test Loss: 1.535784, Test Acc: 57.29%\n",
      "------------------------------------------------------------\n",
      "Epoch 41/60 - Train Loss: 1.256078, Train Acc: 63.94%\n",
      "            - Test Loss: 1.510458, Test Acc: 57.97%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 57.97%\n",
      "Epoch 42/60 - Train Loss: 1.293874, Train Acc: 62.96%\n",
      "            - Test Loss: 1.543111, Test Acc: 57.30%\n",
      "------------------------------------------------------------\n",
      "Epoch 43/60 - Train Loss: 1.245875, Train Acc: 64.12%\n",
      "            - Test Loss: 1.520289, Test Acc: 57.68%\n",
      "------------------------------------------------------------\n",
      "Epoch 44/60 - Train Loss: 1.230332, Train Acc: 64.44%\n",
      "            - Test Loss: 1.498783, Test Acc: 58.32%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 58.32%\n",
      "Epoch 45/60 - Train Loss: 1.220734, Train Acc: 65.08%\n",
      "            - Test Loss: 1.504192, Test Acc: 58.78%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 58.78%\n",
      "Epoch 46/60 - Train Loss: 1.245688, Train Acc: 64.31%\n",
      "            - Test Loss: 1.521508, Test Acc: 58.26%\n",
      "------------------------------------------------------------\n",
      "Epoch 47/60 - Train Loss: 1.195627, Train Acc: 65.57%\n",
      "            - Test Loss: 1.478322, Test Acc: 59.09%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 59.09%\n",
      "Epoch 48/60 - Train Loss: 1.185249, Train Acc: 66.04%\n",
      "            - Test Loss: 1.471912, Test Acc: 59.52%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 59.52%\n",
      "Epoch 49/60 - Train Loss: 1.178968, Train Acc: 65.99%\n",
      "            - Test Loss: 1.458462, Test Acc: 59.69%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 59.69%\n",
      "Epoch 50/60 - Train Loss: 1.164135, Train Acc: 66.48%\n",
      "            - Test Loss: 1.483067, Test Acc: 59.27%\n",
      "------------------------------------------------------------\n",
      "Epoch 51/60 - Train Loss: 1.169017, Train Acc: 66.14%\n",
      "            - Test Loss: 1.481937, Test Acc: 59.29%\n",
      "------------------------------------------------------------\n",
      "Epoch 52/60 - Train Loss: 1.175785, Train Acc: 65.88%\n",
      "            - Test Loss: 1.491753, Test Acc: 59.53%\n",
      "------------------------------------------------------------\n",
      "Epoch 53/60 - Train Loss: 1.187139, Train Acc: 65.93%\n",
      "            - Test Loss: 1.497091, Test Acc: 59.24%\n",
      "------------------------------------------------------------\n",
      "Epoch 54/60 - Train Loss: 1.164217, Train Acc: 66.22%\n",
      "            - Test Loss: 1.487077, Test Acc: 59.10%\n",
      "------------------------------------------------------------\n",
      "Epoch 55/60 - Train Loss: 1.153547, Train Acc: 67.02%\n",
      "            - Test Loss: 1.481391, Test Acc: 59.39%\n",
      "------------------------------------------------------------\n",
      "Epoch 56/60 - Train Loss: 1.137032, Train Acc: 67.21%\n",
      "            - Test Loss: 1.456427, Test Acc: 60.08%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 60.08%\n",
      "Epoch 57/60 - Train Loss: 1.136143, Train Acc: 67.16%\n",
      "            - Test Loss: 1.496744, Test Acc: 59.71%\n",
      "------------------------------------------------------------\n",
      "Epoch 58/60 - Train Loss: 1.149660, Train Acc: 66.83%\n",
      "            - Test Loss: 1.483998, Test Acc: 59.50%\n",
      "------------------------------------------------------------\n",
      "Epoch 59/60 - Train Loss: 1.101117, Train Acc: 68.15%\n",
      "            - Test Loss: 1.454895, Test Acc: 60.57%\n",
      "------------------------------------------------------------\n",
      "New best model saved with Test Acc: 60.57%\n",
      "Epoch 60/60 - Train Loss: 1.110472, Train Acc: 67.96%\n",
      "            - Test Loss: 1.471916, Test Acc: 59.86%\n",
      "------------------------------------------------------------\n",
      "Training complete. Best Test Accuracy: 60.57%\n"
     ]
    }
   ],
   "source": [
    "best_test_accuracy = 0.0  # Initialize the best accuracy\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images_batch, labels_batch) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()  # Clear the gradients\n",
    "        outputs = model(images_batch)  # Forward pass\n",
    "        loss = criterion(outputs, labels_batch)  # Calculate loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "    # Evaluate the model\n",
    "    train_accuracy, train_loss = evaluate(model, train_loader, criterion)\n",
    "    test_accuracy, test_loss = evaluate(model, test_loader, criterion)\n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "    print(f'Epoch {epoch+1:02d}/{num_epochs:02d} - Train Loss: {train_loss:.6f}, Train Acc: {train_accuracy:.2f}%')\n",
    "    print(f'            - Test Loss: {test_loss:.6f}, Test Acc: {test_accuracy:.2f}%')\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Save model if test accuracy improves\n",
    "    if test_accuracy > best_test_accuracy:\n",
    "        best_test_accuracy = test_accuracy\n",
    "        torch.save(model.state_dict(), 'best_model.pth')  # Save the model\n",
    "        print(f\"New best model saved with Test Acc: {best_test_accuracy:.2f}%\")\n",
    "\n",
    "print(f\"Training complete. Best Test Accuracy: {best_test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MarkKogan\\AppData\\Local\\Temp\\ipykernel_12616\\1651938029.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"best_model.pth\")  # Load the state_dict\n",
      "C:\\Users\\MarkKogan\\AppData\\Local\\Temp\\ipykernel_12616\\1651938029.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth', map_location=torch.device('cpu')))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load test data\n",
    "test_data = pd.read_csv('Given_CSV/test.csv')\n",
    "\n",
    "ids = test_data['ID']\n",
    "pixels = test_data.drop(columns=['ID']).values\n",
    "pixels = pixels.reshape(-1, 3, 32, 32)\n",
    "\n",
    "# Apply the reverse normalization to the pixel data\n",
    "pixels_tensor = torch.stack([torch.tensor(img, dtype=torch.float32) for img in pixels])\n",
    "\n",
    "# Create DataLoader\n",
    "test_dataset = TensorDataset(pixels_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Step 4: Make predictions\n",
    "model = Cifar100Classifier()\n",
    "state_dict = torch.load(\"best_model.pth\")  # Load the state_dict\n",
    "model.load_state_dict(torch.load('best_model.pth', map_location=torch.device('cpu')))\n",
    "        # Apply the weights to your model\n",
    "model.eval()                              # Set to evaluation mode\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = batch[0].float()\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get the index of the max value for each prediction\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Step 5: Prepare the result DataFrame\n",
    "result_df = pd.DataFrame({\n",
    "    'ID': ids,\n",
    "    'LABEL': predictions\n",
    "})\n",
    "\n",
    "# Step 6: Save to a new CSV\n",
    "result_df.to_csv('predictions.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "csv_path = './Given_CSV/test.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"Total samples in CSV: {len(df)}\")\n",
    "\n",
    "# Function to reverse the normalization\n",
    "def unnormalize(tensor, mean, std):\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor\n",
    "\n",
    "cifar100_mean = (0.5071, 0.4867, 0.4408)\n",
    "cifar100_std = (0.2675, 0.2565, 0.2761)\n",
    "\n",
    "# Select a few samples to display (e.g., first 5)\n",
    "num_samples = 10\n",
    "samples = df.head(n=num_samples)  # Randomly select samples for diversity\n",
    "\n",
    "# Iterate through the selected samples and display images\n",
    "for index, row in samples.iterrows():\n",
    "    # Extract pixel data and convert to numpy array\n",
    "    pixel_data = row[[f'pixel_{i}' for i in range(1, 3073)]].values.astype(np.float32)\n",
    "\n",
    "    # Reshape to (3, 32, 32)\n",
    "    image = pixel_data.reshape(3, 32, 32)\n",
    "\n",
    "    # Convert to torch tensor\n",
    "    image_tensor = torch.tensor(image)\n",
    "\n",
    "    # Reverse the normalization\n",
    "    image_tensor = unnormalize(image_tensor, cifar100_mean, cifar100_std)\n",
    "\n",
    "    # Clip the values to [0, 1] range\n",
    "    image_tensor = torch.clamp(image_tensor, 0, 1)\n",
    "\n",
    "    # Convert to numpy array and transpose to (32, 32, 3) for plotting\n",
    "    image_np = image_tensor.numpy().transpose(1, 2, 0)\n",
    "\n",
    "    # Convert to PIL Image for better handling (optional)\n",
    "    image_pil = Image.fromarray((image_np * 255).astype(np.uint8))\n",
    "\n",
    "    # Display the image using matplotlib\n",
    "    plt.figure(figsize=(2,2))\n",
    "    plt.imshow(image_pil)\n",
    "    plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100_label_names = [\n",
    "    \"apple\", \"aquarium_fish\", \"baby\", \"bear\", \"beaver\", \"bed\", \"bee\", \"beetle\", \"bicycle\", \"bottle\", \"bowl\", \"boy\", \"bridge\", \"bus\", \"butterfly\", \"camel\", \"can\", \"castle\", \"caterpillar\", \"cattle\", \"chair\", \"chimpanzee\", \"clock\", \"cloud\", \"cockroach\", \"couch\", \"crab\", \"crocodile\", \"cup\", \"dinosaur\", \"dolphin\", \"elephant\", \"flatfish\", \"forest\", \"fox\", \"girl\", \"hamster\", \"house\", \"kangaroo\", \"keyboard\", \"lamp\", \"lawn_mower\", \"leopard\", \"lion\", \"lizard\", \"lobster\", \"man\", \"maple_tree\", \"motorcycle\", \"mountain\", \"mouse\", \"mushroom\", \"oak_tree\", \"orange\", \"orchid\", \"otter\", \"palm_tree\", \"pear\", \"pickup_truck\", \"pine_tree\", \"plain\", \"plate\", \"poppy\", \"porcupine\", \"possum\", \"rabbit\", \"raccoon\", \"ray\", \"road\", \"rocket\", \"rose\", \"sea\", \"seal\", \"shark\", \"shrew\", \"skunk\", \"skyscraper\", \"snail\", \"snake\", \"spider\", \"squirrel\", \"streetcar\", \"sunflower\", \"sweet_pepper\", \"table\", \"tank\", \"telephone\", \"television\", \"tiger\", \"tractor\", \"train\", \"trout\", \"tulip\", \"turtle\", \"wardrobe\", \"whale\", \"willow_tree\", \"wolf\", \"woman\", \"worm\"\n",
    "]\n",
    "label_mapping_df = pd.DataFrame({\n",
    "    'LABEL': range(100),\n",
    "    'Class Name': cifar100_label_names\n",
    "})\n",
    "desired_ids = [23,3,90,51,64,93,57,72]\n",
    "filtered_df = label_mapping_df[label_mapping_df['LABEL'].isin(desired_ids)]\n",
    "\n",
    "# Print the filtered rows\n",
    "print(filtered_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
